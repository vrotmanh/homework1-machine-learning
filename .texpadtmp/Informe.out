\BOOKMARK [1][-]{section.1}{DigitRecognizer}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{b\) Write a function to display an MNIST digit. Display one of each digit}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{c\) Examine the prior probability of the classes in the training data. Is it uniform across the digits? Display a normalized histogram of digit counts. Is it even?}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{d\) Pick one example of each digit from your training data. Then, for each sample digit, compute and show the best match \(nearest neighbor\) between your chosen sample and the rest of the training data. Use L2 distance between the two images\220 pixel values as the metric. This probably won\220t be perfect, so add an asterisk next to the erroneous examples \(if any\).}{section.1}% 4
\BOOKMARK [2][-]{subsection.1.4}{e\) Consider the case of binary comparison between the digits 0 and 1. Ignoring all the other digits, compute the pairwise distances for all genuine matches and all impostor matches, again using the L2 norm. Plot histograms of the genuine and impostor distances on the same set of axes.}{section.1}% 5
\BOOKMARK [2][-]{subsection.1.5}{f\) Generate an ROC curve from the above sets of distances. What is the equal error rate? What is the error rate of a classifier that simply guesses randomly?}{section.1}% 6
\BOOKMARK [2][-]{subsection.1.6}{g\) Implement a K-NN classifier. \(You cannot use external libraries for this question; it should be your own implementation.\)}{section.1}% 7
\BOOKMARK [2][-]{subsection.1.7}{h\) Using the training data for all digits, perform 3 fold cross-validation on your K-NN classifier and report your average accuracy.}{section.1}% 8
\BOOKMARK [2][-]{subsection.1.8}{i\) Generate a confusion matrix \(ofsize 10 x 10\) from your results. Which digits are particularly tricky to classify?}{section.1}% 9
\BOOKMARK [1][-]{section.2}{The Titanic Disaster}{}% 10
\BOOKMARK [1][-]{section.3}{Written Exercises}{}% 11
\BOOKMARK [2][-]{subsection.3.1}{Variance of a sum. Show that the variance of a sum is var[X-Y]=var[X] + var[Y] - 2*cov[X,Y], where cov[X,Y] is the covariance between random variables X and Y.}{section.3}% 12
\BOOKMARK [2][-]{subsection.3.2}{Bayes rule for quality control. You\220re the foreman at a factory making ten million widgets per year. As a quality control step before shipment, you create a detector that tests for defective widgets be- fore sending them to customers. The test is uniformly 95\045 accurate, meaning that the probability of testing positive given that the widget is defective is 0.95, as is the probability of testing negative given that the widget is not defective. Further, only one in 100,000 widgets is actually defective.}{section.3}% 13
\BOOKMARK [2][-]{subsection.3.3}{In k-nearest neighbors, the classification is achieved by majority vote in the vicinity of data. Sup- pose our training data comprises n data points with two classes, each comprising exactly half of the training data, with some overlap between the two classes.}{section.3}% 14
